{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Classifier comparison\n\nA comparison of a several classifiers in :mod:`imbens.ensemble` \non synthetic datasets. The point of this example is to illustrate the nature \nof decision boundaries of different imbalanced ensmeble classifiers. \nThis should be taken with a grain of salt, as the intuition conveyed by these \nexamples does not necessarily carry over to real datasets.\n\nThe plots show training points in solid colors and testing points semi-transparent. \nThe lower right shows the average precision score (AUPRC) on the test set.\n\nThis example uses:\n    \n    - Reweighting-based method\n        - :class:`imbens.ensemble.AdaCostClassifier`\n        - :class:`imbens.ensemble.AdaUBoostClassifier`\n        - :class:`imbens.ensemble.AsymBoostClassifier`\n    - Under-sampling-based method\n        - :class:`imbens.ensemble.SelfPacedEnsembleClassifier`\n        - :class:`imbens.ensemble.BalanceCascadeClassifier`\n        - :class:`imbens.ensemble.BalancedRandomForestClassifier`\n        - :class:`imbens.ensemble.EasyEnsembleClassifier`\n        - :class:`imbens.ensemble.RUSBoostClassifier`\n        - :class:`imbens.ensemble.UnderBaggingClassifier`\n    - Over-sampling-based method\n        - :class:`imbens.ensemble.OverBoostClassifier`\n        - :class:`imbens.ensemble.SMOTEBoostClassifier`\n        - :class:`imbens.ensemble.KmeansSMOTEBoostClassifier`\n        - :class:`imbens.ensemble.OverBaggingClassifier`\n        - :class:`imbens.ensemble.SMOTEBaggingClassifier`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Zhining Liu <zhining.liu@outlook.com>\n# License: MIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n\n# Import imbalanced-ensemble\nimport imbens\n\n# Import utilities\nimport numpy as np\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom imbens.datasets import make_imbalance\n\n# Import plot utilities\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nRANDOM_STATE = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparation\n**Make 3 imbalanced toy classification tasks.**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "distribution = {0: 100, 1: 50}\n\n# dataset 1\nX, y = make_moons(200, noise=0.2, random_state=RANDOM_STATE)\ndataset1 = make_imbalance(\n    X, y, sampling_strategy=distribution, random_state=RANDOM_STATE\n)\n# dataset 2\nX, y = make_circles(200, noise=0.2, factor=0.5, random_state=RANDOM_STATE)\ndataset2 = make_imbalance(\n    X, y, sampling_strategy=distribution, random_state=RANDOM_STATE\n)\n# dataset 3\nX, y = make_classification(\n    200,\n    n_features=2,\n    n_redundant=0,\n    n_informative=2,\n    random_state=1,\n    n_clusters_per_class=1,\n)\nX += 2 * np.random.RandomState(RANDOM_STATE).uniform(size=X.shape)\ndataset3 = make_imbalance(\n    X, y, sampling_strategy=distribution, random_state=RANDOM_STATE\n)\n\ndatasets = [dataset1, dataset2, dataset3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Load all ensemble classifiers**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from imbens.utils.testing import all_estimators\n\ninit_kwargs = {'n_estimators': 5, 'random_state': RANDOM_STATE}\nall_ensembles_clf = {\n    name: ensemble(**init_kwargs) for (name, ensemble) in all_estimators('ensemble')\n}\n\nprint('{:<30s} | Class \\n{:=<120s}'.format('Method', ''))\nfor (name, ensemble) in all_estimators('ensemble'):\n    print('{:<30s} | {}'.format(name, ensemble))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Function for classifier comparison**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_classifier_comparison(classifiers, names, datasets, figsize):\n\n    h = 0.02  # step size in the mesh\n\n    figure = plt.figure(figsize=figsize)\n    i = 1\n    # iterate over datasets\n    for ds_cnt, ds in enumerate(datasets):\n        # preprocess dataset, split into training and test part\n        X, y = ds\n        X = StandardScaler().fit_transform(X)\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.4, random_state=42\n        )\n\n        x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n        y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n        # just plot the dataset first\n        cm = plt.cm.RdBu\n        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        if ds_cnt == 0:\n            ax.set_title(\"Input data\")\n        # Plot the training points\n        ax.scatter(\n            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors='k'\n        )\n        # Plot the testing points\n        ax.scatter(\n            X_test[:, 0],\n            X_test[:, 1],\n            c=y_test,\n            cmap=cm_bright,\n            alpha=0.6,\n            edgecolors='k',\n        )\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        i += 1\n\n        # iterate over classifiers\n        for name, clf in zip(names, classifiers):\n            ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n            clf.fit(X_train, y_train)\n            score = sklearn.metrics.average_precision_score(y_test, clf.predict(X_test))\n\n            # Plot the decision boundary. For that, we will assign a color to each\n            # point in the mesh [x_min, x_max]x[y_min, y_max].\n            if hasattr(clf, \"decision_function\"):\n                Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n            else:\n                Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n            # Put the result into a color plot\n            Z = Z.reshape(xx.shape)\n            ax.contourf(xx, yy, Z, cmap=cm, alpha=0.8)\n\n            # Plot the training points\n            ax.scatter(\n                X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors='k'\n            )\n            # Plot the testing points\n            ax.scatter(\n                X_test[:, 0],\n                X_test[:, 1],\n                c=y_test,\n                cmap=cm_bright,\n                edgecolors='k',\n                alpha=0.6,\n            )\n\n            ax.set_xlim(xx.min(), xx.max())\n            ax.set_ylim(yy.min(), yy.max())\n            ax.set_xticks(())\n            ax.set_yticks(())\n            if ds_cnt == 0:\n                ax.set_title(name)\n            ax.text(\n                0.95,\n                0.06,\n                ('%.2f' % score).lstrip('0'),\n                size=15,\n                bbox=dict(boxstyle='round', alpha=0.8, facecolor='white'),\n                transform=ax.transAxes,\n                horizontalalignment='right',\n            )\n            i += 1\n\n    plt.tight_layout()\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare all under-sampling-based ensemble algorithms\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from imbens.ensemble._under_sampling.__init__ import __all__ as names\n\nclassifiers = [all_ensembles_clf[name] for name in names]\nplot_classifier_comparison(\n    classifiers, names, datasets, figsize=(len(names) * 3 + 3, 9)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare all over-sampling-based ensemble algorithms\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from imbens.ensemble._over_sampling.__init__ import __all__ as names\n\nclassifiers = [all_ensembles_clf[name] for name in names]\nplot_classifier_comparison(\n    classifiers, names, datasets, figsize=(len(names) * 3 + 3, 9)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare all reweighting-based ensemble algorithms\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from imbens.ensemble._reweighting.__init__ import __all__ as names\n\nclassifiers = [all_ensembles_clf[name] for name in names]\nplot_classifier_comparison(\n    classifiers, names, datasets, figsize=(len(names) * 3 + 3, 9)\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}