{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Use dynamic resampling schedule\n\nThis example demonstrates how to customize the sampling schedule to achieve dynamic resampling.\nThis can be easily done by setting the \"balancing_schedule\" parameter when calling the \"fit()\" method. \n\nNote that this feature only applies to resampling-based ensemble classifiers that are iteratively trained.\n\nThis example uses:\n\n    - :class:`imbalanced_ensemble.ensemble.SelfPacedEnsembleClassifier`\n    - :class:`imbalanced_ensemble.ensemble.SMOTEBoostClassifier`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Zhining Liu <zhining.liu@outlook.com>\n# License: MIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n\n# Import imbalanced_ensemble\nimport imbalanced_ensemble as imbens\n\n# Import utilities\nimport math\nimport sklearn\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom imbalanced_ensemble.ensemble.base import sort_dict_by_key\nfrom collections import Counter\n\nRANDOM_STATE = 42\n\ninit_kwargs = {\n    'n_estimators': 5,\n    'random_state': RANDOM_STATE,\n}\nfit_kwargs = {\n    'train_verbose': {\n        'granularity': 1,\n        'print_metrics': False,\n    },\n}\n\n# sphinx_gallery_thumbnail_number = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare data\nMake a toy 3-class imbalanced classification task.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# make dataset\nX, y = make_classification(n_classes=3, class_sep=2,\n    weights=[0.1, 0.3, 0.6], n_informative=3, n_redundant=1, flip_y=0,\n    n_features=20, n_clusters_per_class=2, n_samples=2000, random_state=0)\n\n# train valid split\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.5, stratify=y, random_state=RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the original class/marginal distribution P(Y) of the training data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Original training dataset distribution %s' % sort_dict_by_key(Counter(y_train)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Uniform under-sampling\nBy default, under-sampling-based ensemble methods will consider the smallest class as the minority class (class 0 with 100 samples).  \nAll other classes (class 1 and 2) will be considered as majority classes and will be under-sampled until the number of samples is equalized.  \n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take ``SelfPacedEnsembleClassifier`` as example\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spe_clf = imbens.ensemble.SelfPacedEnsembleClassifier(**init_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Train with the default under-sampling setting**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spe_clf.fit(X_train, y_train, **fit_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Progressive under-sampling\nThe resample class distributions are progressive \ninterpolation between the original and the target class distribution.\nExample: For a class $c$, say the number of samples is $N_{c}$ \nand the target number of samples is $N'_{c}$. Suppose that we are \ntraining the $t$-th base estimator of a $T$-estimator ensemble, then \nwe expect to get $(1-\\frac{t}{T}) \\cdot N_{c} + \\frac{t}{T} \\cdot N'_{c}$ \nsamples after resampling;\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Train with progressive under-sampling schedule**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spe_clf.fit(X_train, y_train,\n    balancing_schedule='progressive', # Progeressive under-sampling\n    **fit_kwargs\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define your own resampling schedule. \nYour schedule function should take 4 positional arguments with order (``'origin_distr'``: \n``dict``, ``'target_distr'``: ``dict``, ``'i_estimator'``: ``int``, ``'total_estimator'``: \n``int``), and returns a ``'result_distr'``: ``dict``. For all parameters of type ``dict``, \nthe keys of type ``int`` correspond to the targeted classes, and the values of type ``str`` \ncorrespond to the (desired) number of samples for each class.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Train with user-defined dummy resampling schedule**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def my_dummy_schedule(origin_distr:dict, target_distr:dict, \n                      i_estimator:int, total_estimator:int):\n    '''A dummy resampling schedule'''\n    return origin_distr\n\nspe_clf.fit(X_train, y_train,\n    balancing_schedule=my_dummy_schedule, # Use your progressive resampling schedule\n    **fit_kwargs\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Train with user-defined progressive resampling schedule**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def my_progressive_schedule(origin_distr:dict, target_distr:dict, \n                            i_estimator:int, total_estimator:int):\n    '''A user-defined progressive resampling schedule'''\n    # compute training progress\n    p = i_estimator / (total_estimator-1) if total_estimator >= 1 else 1\n    result_distr = {}\n    # compute expected number of samples for each class\n    for label in origin_distr.keys():\n        result_distr[label] = math.ceil(\n            origin_distr[label] * (1-p) + target_distr[label] * p - 1e-10 # for numerical stability\n        )\n    return result_distr\n\n\nspe_clf.fit(X_train, y_train,\n    balancing_schedule=my_progressive_schedule, # Use your progressive resampling schedule\n    **fit_kwargs\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Over-sampling\nBy default, over-sampling-based ensemble methods will consider the largest class as the majority class (class 2 with 600 samples).  \nAll other classes (class 0 and 1) will be considered as minority classes and will be over-sampled until the number of samples is equalized.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The over-sampling schedule can be customized in the same way as under-sampling.**\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take ``SMOTEBoostClassifier`` as example\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "smoteboost_clf = imbens.ensemble.SMOTEBoostClassifier(**init_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Train with the default over-sampling setting**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "smoteboost_clf.fit(X_train, y_train, **fit_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Train with progressive over-sampling schedule**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "smoteboost_clf.fit(X_train, y_train, balancing_schedule='progressive', **fit_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Train with user-defined dummy resampling schedule**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "smoteboost_clf.fit(X_train, y_train,\n    balancing_schedule=my_dummy_schedule,\n    **fit_kwargs\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Train with user-defined progressive resampling schedule**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "smoteboost_clf.fit(X_train, y_train,\n    balancing_schedule=my_progressive_schedule,\n    **fit_kwargs\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize different resampling schedule\nImplement some plot utilities\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imbalanced_ensemble.utils._plot import set_ax_border\n\nylim = (0, 630)\n\ndef plot_class_distribution(distr:dict, xlabel:str='Class Label', \n                            ylabel:str='Number of samples', **kwargs):\n    distr = dict(sorted(distr.items(), key=lambda k: k[0], reverse=True))\n    ax = sns.barplot(\n        x=list(distr.keys()), \n        y=list(distr.values()),\n        order=list(distr.keys()),\n        **kwargs\n    )\n    set_ax_border(ax)\n    ax.grid(axis='y', alpha=0.5, ls='-.')\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    return ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Original class distribution**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = plot_class_distribution(spe_clf.origin_distr_)\nax.set_title('Original imbalanced class distribution')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Uniform under/over-sampling**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n\nplot_class_distribution(spe_clf.target_distr_, ax=ax1)\nax1.set(ylim=ylim, title='After uniform under-sampling')\nplot_class_distribution(smoteboost_clf.target_distr_, ax=ax2)\nax2.set(ylim=ylim, title='After uniform over-sampling')\n\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Progressive under/over-sampling**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from imbalanced_ensemble.utils._validation_param import _progressive_schedule\n\nN = 10\ni_estimators = [0, 4, 9]\norigin_distr = sort_dict_by_key(Counter(y_train))\nunder_distr = spe_clf.target_distr_\nover_distr = smoteboost_clf.target_distr_\n\nfig, axes = plt.subplots(2, 3, figsize=(9, 5))\n\n# Progressive under-sampling\nfor ax, i in zip(axes[0], i_estimators):\n    resample_distr = _progressive_schedule(origin_distr, under_distr, i, N)\n    plot_class_distribution(resample_distr, ax=ax)\n    ax.set(ylim=ylim, title=f'After prog US @Iter {i+1}/{N}')\n\n# Progressive over-sampling\nfor ax, i in zip(axes[1], i_estimators):\n    resample_distr = _progressive_schedule(origin_distr, over_distr, i, N)\n    plot_class_distribution(resample_distr, ax=ax)\n    ax.set(ylim=ylim, title=f'After prog OS @Iter {i+1}/{N}')\n\nfig.suptitle(\"Abbreviation: prog: progressive; US: under-sampling; OS: over-sampling; Iter: iteration.\", \n             y=0.02, style='italic')\nfig.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}